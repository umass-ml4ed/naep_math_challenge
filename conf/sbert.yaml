name : "" #'train' # name for the experiment
#directory information
train_path : 'data/train.csv' # the path of train dataset
save_dir : 'data/' # The directory to save result
save_model_dir : 'saved_models/' # The directory to save models
all_predict_path: ''   #directtory that save all of the prediction information
reduce_path: ''

#training and testing information
split :  True # no testing dataset random split 'training dataset as train(8/10)/val(1/10)/test(1/10)')
test_fold: 9 # use fold 9 for testing dataset
val_fold: 8 # use fold 8 for validation dataset
test_path : 'data/test.csv' # the path of train dataset

#different mode:
in_context :  True #  'The input will include in context information
base :  False # basic classification taskã€€'\ninput:  response \noutput:  label ')
eval_only: False #skip training part, only do the evaluation
embedding: False #True for also save a file for embedding
analysis: False
reduce: False #if sample reduced dataset
same: False
random: False

# model definition
lm : 'bert-base-uncased' # Base Language model
tok : 'bert-base-uncased' # Base Language model tokenizer
ag: False #auto regressive
e2e: True

task : 'all' # train on all task

#2.in context
closed_form :  True #  'add closed form response to the input
question_id :  False # add question id information to the input
examples :  True # 'add examples') #random categorise K
n_examples : 2 #  'Num of examples for each score category
prompting: False
num_label: Fasle

#2.2 Retriever setting
retriever:
  name: ""
  load_outside: True
  encoderModel: False #bert-base-uncased
  batch_size: 32
  k: 20
  exclude_construct_level: False
  sample_size: 20

#3 analysis setting
ana:
  save_top_k: True


im_balance: False #if turn on mode to solve im balance labeling issue

#3 Model architecture
multi_model: False #if true, train separate model for each item
multi_head: False



#label information
label: 0  #  'different type of labels:
# 0:  simple label without specifications (e.g. 123)
# 1:  detailed label with specifications (e.g. 1, 1A, 1B 2A, 2B, 3')
# 2:  Reduced label. 3, 2A -> 2, 2B, 1 -> 1 only works for type 2,3

#loss information
loss: 0
# 0 cross entropy
# 1 oll loss

# optimizer params
lr_schedule : 'warmup-const'
opt : 'adam' # Optimizer to use' choices=['sgd' 'adam' 'lars'
iters : 21  # number of epochs
warmup : 0  # number of warmup iterations in proportion to \'iters\'
lr : 2e-5  # base learning rate
batch_size : 32  # batch size
decay : 0.01

# trainer params
save_freq : 1  # epoch frequency to save the model
eval_freq : 1  # epoch frequency for evaluation
workers : 1  # number of data loader workers
seed : 42  # random seed
sample_seed: -1

# dataset params


# evaluation params
best_metric : 'kappa'  # choose validation data
# training params
eval :  False # only for evaluation
cuda :  True # use cuda
save :  False # save model every save_freq epochs
debug :  False # debug mode with less data
gpu_num: 1

#extra
num_questions: 11

#wandb params
logging:
  record : False
  project: "naep-math-challenge"
  entity: "ml4ed"
  debug: True
  reboase: False
  group: ~